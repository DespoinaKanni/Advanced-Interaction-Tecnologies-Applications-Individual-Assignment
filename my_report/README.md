# Lesson: Advanced Interaction Technologies & Applications

### First and Last Name: Δέσποινα Καννή Ρεμπούλη
### University Registration Number: dpsd17042
### GitHub Personal Profile: https://github.com/DespoinaKanni
### Advanced Interaction Tecnologies & Applications Github Personal Repository: https://github.com/DespoinaKanni/Advanced-Interaction-Tecnologies-Applications-Individual-Assignment

# Introduction

# Summary


# 1st Deliverable

Η εργασία έγινε στους υπολογιστές της σχολής. 

**`Βήματα που πραγματοποίησα πριν ξεκινήσω τις εργασίες:`**

1. Συνδέθηκα στο GitΗub με τους κωδικούς μου.
2. Μπήκα στο GitHub Desktop, έβαλα τους κωδικούς του GitHub και συνδέθηκε με τον λογαριασμό μου. 
3. Από το GitHub Desktop έκανα clone το repository του Advanced Interaction Tecnologies & Applications και το αποθήκευσα μέσα στον φάκελο του processing.

## Video Capture

Για το πρώτο παραδοτέο έφτιαξα ένα νεό αρχείο και το αποθήκευσα μέσα στον φάκελο του video capture στο source. 
Στο αρχείο έφτιαξα στο preferences την διαδρομή ώστε να είναι στο φάκελο του processing στο data.  

Μπήκα στο *eclass* και άνοιξα το βιβλίο `Shiffman, Daniel - Learning Processing, Second Edition_ A Beginner's Guide to Progra.pdf`.
Βρήκα το παράδειγμα 16_1 και το αντέγραψα στο νέο άρχειο.

![Capture](https://user-images.githubusercontent.com/100956507/199835802-c4565bb6-0f5b-4325-9bcd-fed6de8c702d.PNG)

## Recorded Video

* `Άνοιξα τα έτοιμα παραδείγματα 16_4 και 16_5 από τα libraries και τα έτρεξα.` 

![video](https://user-images.githubusercontent.com/100956507/199837756-066e5f2d-cf8c-42cd-9e0e-c3e68d805b2d.PNG)

![video2](https://user-images.githubusercontent.com/100956507/199838205-f534bac5-fc7c-469b-a0fe-3e7865b24238.PNG)

* Βρήκα το excerise 16_2 μέσα από το βιβλίο και πάτησα το [link](https://processing.org/reference/libraries/video/Movie_speed_.html) που περιέχει.

![exersice](https://user-images.githubusercontent.com/100956507/199838714-a4013ecf-552c-4262-8fae-7caca9a54b5f.PNG)

* `Ο κώδικας από το παραπάνω link` 

![code](https://user-images.githubusercontent.com/100956507/199839741-bb5697b5-fbbb-4c5d-902d-48bbfe14823c.PNG)

* Προσάρμοσα τα παραδείγματα με τον κώδικα που με οδήγησε το link ώστε να μπορεί να παίζει το δικό μου βίντεο και να μπορώ να του αλλάζω την ταχύτητα με το ποντίκι.

* Βρήκα ένα βίντεο από το youtube και από την [ιστοσελίδα](https://y2mate.tools/en57ef) το κατέβασα. 
Έπειτα έφτιαξα έναν φάκελο μέσα στον φάκελο που είχα αποθηκεύσει το αρχείο μου και τον ονόμασα data. Εκεί αποθήκευσα το βίντεο. Τέλος έβαλα στον κώδικα το όνομα του βίντεο μου ώστε να μου το εμφανίζει.

## QR Code

* Άνοιξα το έτοιμο παράδειγμα 15_1 για να καταλάβω πως εμφανίζω εικόνες.

![imgparadeigma](https://user-images.githubusercontent.com/100956507/199845644-5d264076-eb5b-481b-8e27-8ada07b13894.PNG)

* Βρήκα μέσα από την [*βιβλιοθήκη*](https://shiffman.net/p5/qrcode-processing/) του qrcode τον κώδικα.

![librarycode](https://user-images.githubusercontent.com/100956507/199846545-320cd350-4954-40c2-adac-4a3146257332.PNG)

* Συνδύασα τους 2 κώδικες ώστε να εμφανίζεται στην οθόνη η εικόνα του qrcode και στην συνέχεια να την διαβάζει αυτόματα και να με οδηγεί κατευθείαν μέσα στο link.

* Το Qr Code το έφτιαξα από την [ιστοσελίδα](https://qr.io/?gclid=EAIaIQobChMItMOK7YWT-wIVT4KDBx0XhgRKEAAYASAAEgLAePD_BwE).

* Αποθήκευσα μέσα στο data την εικόνα του qr code.

## QR Code - Camera Read

Άνοιξα το έτοιμο παράδειγμα *`QrCodeExample`* και το αντέγραψα σε ένα δικό μου αρχείο. Έσβησα το έτοιμο `case f` και πρόσθεσα ένα δικό μου *`case k`*.
Πάτησα το play και μου άνοιξε η κάμερα, πλησίασα στην κάμερα του υπολογιστή την εικόνα του qr code μου και πάτησα το space μέχρι να μου την διαβάσει. Όταν μου την διάβασε πάτησα το γράμμα `k` και με έβγαλε στο προφιλ μου στο GitHub.

## Augmented Reality

Κατέβασα την βιβλιοθηκή **nyar4psg** από το processing (Java). Αντέγραψα τον φάκελο **data** από την βιβλιοθήκη και τον αποθήκευσα μέσα στο φάκελο που έχω την εργασία μου.
Άνοιξα το πάράδειγμα **SimpleLite**, κατέβασα μια εικόνα της επιλογής μου και την αποθήκευσα μέσα στον φάκελο data. Τέλος, προσάρμοσα στον κώδικα το όνομα της δικιάς μου είκονας. 
Στην συνέχεια άλλαξα το **path** και έβαλα το όνομα του δικού μου αρχείου με απότελεσμα να ανοίγει η κάμερα, να αναγνωρίζει το hiro και να εμφανίζει την εικόνα μου.

**`Από τα 2 hiro που είναι μέσα στον φάκελο data, αυτό που λειτουργεί για να εμφανίζεται η είκονα μου είναι το εξής:`**  

![320x240ABGR](https://user-images.githubusercontent.com/100956507/200048477-a62914cf-187e-4d9b-982d-61d2cb947b87.png)

*Γενικά επειδή η κάμερα δυσκολευόταν να ανοίξει χρειάστηκε να πατάω πολλές φορές το play και το pause του processing μέχρι να την εντοπίσει. Για να μου ανοίγει η κάμερα χωρίς πολλές δοκιμές σε κάποια παραδοτέα πρόσθεσα στον κώδικα `pipeline:autovideosrc`*

# 2nd Deliverable

## Background Removal

**`Για να βάλω είκονα στο background:`**

* Άνοιξα το έτοιμο [Example 16-12](http://learningprocessing.com/examples/chp16/example-16-12-BackgroundRemove) και το [Exercise 16-6](http://learningprocessing.com/exercises/chp16/exercise-16-06-greenscreen) από τα libraries και τα έτρεξα.

* Αντέγραψα και επεξεργάστηκα το Exercise 16-6 σε ένα νέο δικό μου αρχείο και το αποθήκευσα.

* Έφτιαξα έναν νεό φάκελο data όπου εκεί μέσα αποθήκευσα μια εικόνα από το διαδίκτυο.

### *`ΚΩΔΙΚΑΣ:`* 
άλλαξα και έβαλα στον κώδικα το *όνομα* της εικόνας μου. Πήγα στην φωτογραφία και κοίταξα στα properties, στις λεπτομέρειες, τις *διαστάσεις (πλάτος, ύψος)* της. Έβαλα τις διαστάσεις στο *size* ώστε η είκονα να εμφανίζεται ομοιόμορφη από πίσω μου.

Τέλος, 

1. έτρεξα το αρχείο και άνοιξε η κάμερα, 
2. έφυγα τελείως από το οπτικό πεδίο της κάμερας, 
3. πάτησα κλικ με το ποντίκι στο παράθυρο της κάμερας και έτσι εμφανίστηκε η είκονα. 
4. έπειτα μπήκα και εγώ στο οπτικό πεδίο της κάμερας και με έδειχνε μπροστά από την εικόνα. 

**`Για να βάλω video στο background:`**

* Κατέβασα ένα βίντεο από το Youtube από την [ιστοσελίδα](https://y2mate.tools/en57ef).

* Το αποθήκευσα σε ένα καινούργιο φάκελο data.

* Πήγα στο [Example 16-4](http://learningprocessing.com/examples/chp16/example-16-04-MoviePlayback) για να θυμηθώ πως εισάγουμε ένα βίντεο σε επανάληψη και προσάρμοσα τον κώδικα στον δικό μου.

`κώδικας:`

![code16-4](https://user-images.githubusercontent.com/100956507/205693785-8ede01e4-1bc7-4941-8b62-75cfcfc6eb71.PNG)


### *`ΚΩΔΙΚΑΣ:`* 
άλλαξα και έβαλα στον κώδικα το *όνομα* του video μου. Πήγα στην video και κοίταξα στα properties, στις λεπτομέρειες, τις *διαστάσεις (πλάτος ύψος)* του. Έβαλα τις διαστάσεις στο *size* ώστε το video να εμφανίζεται ομοιόμορφα από πίσω μου.


Τέλος, 

1. έτρεξα το αρχείο και άνοιξε η κάμερα, 
2. έφυγα τελείως από το οπτικό πεδίο της κάμερας, 
3. πάτησα κλικ με το ποντίκι στο παράθυρο της κάμερας και έτσι εμφανίστηκε το βίντεο. 
4. έπειτα μπήκα και εγώ στο οπτικό πεδίο της κάμερας και φαινόταν το βίντεο να παίζει από πίσω μου σε επανάληψη. 


![backgroundimage](https://user-images.githubusercontent.com/100956507/205702987-7a216b48-387a-4445-b24d-8a4cc9a6ee87.png)

![backvideo](https://user-images.githubusercontent.com/100956507/205703716-d3e28233-2f23-4d7f-b41f-faf532de5f0e.png)

![back2video](https://user-images.githubusercontent.com/100956507/205705028-e6af3df4-ad5d-4141-8f91-c5e3e97c61ee.png)


## Motion Detection

Αντέγραψα και επεξεργάστηκα το [Exercise 16-7](http://learningprocessing.com/exercises/chp16/exercise-16-07-track-motion) σε ένα νέο δικό μου αρχείο και το αποθήκευσα. Άλλαξα στον κώδικα το *`threshold, το smooth, το χρώμα - fill και το μέγεθος της έλλειψη - ellipse.`* Στην συνέχεια έτρεξα το προγράμμα και η έλλειψη ακολουθούσε το χέρι μου και γενικά τις κινήσεις μου.

![motion1](https://user-images.githubusercontent.com/100956507/205711684-4dbae8c5-30ae-4af7-89d1-8940179a2235.png)


## Background Substraction - Library use

Κατέβασα την βιβλιοθήκη OpenCV for Processing και άνοιξα το έτοιμο παράδειγμα BackgroundSubstraction. Έσβησα ότι εντολές αφορούσαν το βίντεο που είχε μέσα και πρόσθεσα στον κώδικα να ανοίγει η κάμερα του υπολογιστή από το παράδειγμα του Video Capture. Έπειτα, άλλαξα το χρώμα του περιγράμματος (stroke) και έτρεξα το αρχείο. Ανίχνευε τις κινήσεις μου, τοποθετώντας τες μέσα σε περιγράμματα και γραμμές.

![βιβλιοθηκη](https://user-images.githubusercontent.com/100956507/205720155-0010daef-e3f8-44bf-9822-fba0b28f544a.png)

![Capture](https://user-images.githubusercontent.com/100956507/199835802-c4565bb6-0f5b-4325-9bcd-fed6de8c702d.PNG)

![Screenshot (18)](https://user-images.githubusercontent.com/100956507/205722111-334c5fa1-a52d-4053-b718-68b8f1dd57f8.png)

![Screenshot (19)](https://user-images.githubusercontent.com/100956507/205722133-5c867ae1-17b8-485b-b117-d2d2525346b6.png)

![Screenshot (20)](https://user-images.githubusercontent.com/100956507/205722178-fbaac0a9-472c-4a03-8818-a0f53c8d8c58.png)



## Object Tracking

Αντέγραψα και επεξεργάστηκα το [Exercise 16-5](http://learningprocessing.com/exercises/chp16/exercise-16-05-snake-tracking) σε ένα νέο δικό μου αρχείο και το αποθήκευσα. Πήγα στην κλάση του `snake` και στο `χρώμα - fill έβαλα να εντοπίζει το χρώμα από την κάμερα`. Έτρεξα το προγράμμα και έδειξα στην κάμερα ένα μαρκαδόρο με πράσινο καπάκι, κλίκαρα πάνω στο πράσινο καπάκι και οι κύκλοι που εμφανίστηκαν στην οθόνη ήταν και αυτοί πράσινοι, ενώ στο σγουγγάρι που η επιφάνειά του ήταν χρώμα μπλέ, οι κύκλοι που εμφανίστηκαν στην οθόνη ήταν και αυτοί μπλε. Όσο κουνούσα τον μαρκαδόρο οι κύκλοι τον ακολουθούσαν δημιουργώντας μια "ουρά".

![track1](https://user-images.githubusercontent.com/100956507/205713105-ed647215-2b64-4d82-92db-b995e683d942.png)

![track2](https://user-images.githubusercontent.com/100956507/205713134-183cd61b-0533-42ed-bd1a-9baab5c50791.png)



# 3rd Deliverable 


# Bonus 


# Conclusions


# Sources
